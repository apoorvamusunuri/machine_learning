{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62044222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c66283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106643</td>\n",
       "      <td>3.59437</td>\n",
       "      <td>132.8040</td>\n",
       "      <td>3.18428</td>\n",
       "      <td>0.081971</td>\n",
       "      <td>1.18859</td>\n",
       "      <td>3.73238</td>\n",
       "      <td>2.266270</td>\n",
       "      <td>2.09959</td>\n",
       "      <td>0.012330</td>\n",
       "      <td>...</td>\n",
       "      <td>1.09862</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>-0.011715</td>\n",
       "      <td>0.052759</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>4.211250</td>\n",
       "      <td>1.97877</td>\n",
       "      <td>0.085974</td>\n",
       "      <td>0.240496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125021</td>\n",
       "      <td>1.67336</td>\n",
       "      <td>76.5336</td>\n",
       "      <td>3.37825</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>5.09366</td>\n",
       "      <td>1.27562</td>\n",
       "      <td>-0.471318</td>\n",
       "      <td>4.54594</td>\n",
       "      <td>0.037706</td>\n",
       "      <td>...</td>\n",
       "      <td>3.46017</td>\n",
       "      <td>0.017054</td>\n",
       "      <td>0.124863</td>\n",
       "      <td>0.154064</td>\n",
       "      <td>0.606848</td>\n",
       "      <td>-0.267928</td>\n",
       "      <td>2.57786</td>\n",
       "      <td>-0.020877</td>\n",
       "      <td>0.024719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036330</td>\n",
       "      <td>1.49747</td>\n",
       "      <td>233.5460</td>\n",
       "      <td>2.19435</td>\n",
       "      <td>0.026914</td>\n",
       "      <td>3.12694</td>\n",
       "      <td>5.05687</td>\n",
       "      <td>3.849460</td>\n",
       "      <td>1.80187</td>\n",
       "      <td>0.056995</td>\n",
       "      <td>...</td>\n",
       "      <td>4.88300</td>\n",
       "      <td>0.085222</td>\n",
       "      <td>0.032396</td>\n",
       "      <td>0.116092</td>\n",
       "      <td>-0.001688</td>\n",
       "      <td>-0.520069</td>\n",
       "      <td>2.14112</td>\n",
       "      <td>0.124464</td>\n",
       "      <td>0.148209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014077</td>\n",
       "      <td>0.24600</td>\n",
       "      <td>779.9670</td>\n",
       "      <td>1.89064</td>\n",
       "      <td>0.006948</td>\n",
       "      <td>1.53112</td>\n",
       "      <td>2.69800</td>\n",
       "      <td>4.517330</td>\n",
       "      <td>4.50332</td>\n",
       "      <td>0.123494</td>\n",
       "      <td>...</td>\n",
       "      <td>3.47439</td>\n",
       "      <td>-0.017103</td>\n",
       "      <td>-0.008100</td>\n",
       "      <td>0.062013</td>\n",
       "      <td>0.041193</td>\n",
       "      <td>0.511657</td>\n",
       "      <td>1.96860</td>\n",
       "      <td>0.040017</td>\n",
       "      <td>0.044873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003259</td>\n",
       "      <td>3.71542</td>\n",
       "      <td>156.1280</td>\n",
       "      <td>2.14772</td>\n",
       "      <td>0.018284</td>\n",
       "      <td>2.09859</td>\n",
       "      <td>4.15492</td>\n",
       "      <td>-0.038236</td>\n",
       "      <td>3.37145</td>\n",
       "      <td>0.034166</td>\n",
       "      <td>...</td>\n",
       "      <td>1.91059</td>\n",
       "      <td>-0.042943</td>\n",
       "      <td>0.105616</td>\n",
       "      <td>0.125072</td>\n",
       "      <td>0.037509</td>\n",
       "      <td>1.043790</td>\n",
       "      <td>1.07481</td>\n",
       "      <td>-0.012819</td>\n",
       "      <td>0.072798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          f0       f1        f2       f3        f4       f5       f6  \\\n",
       "id                                                                     \n",
       "0   0.106643  3.59437  132.8040  3.18428  0.081971  1.18859  3.73238   \n",
       "1   0.125021  1.67336   76.5336  3.37825  0.099400  5.09366  1.27562   \n",
       "2   0.036330  1.49747  233.5460  2.19435  0.026914  3.12694  5.05687   \n",
       "3  -0.014077  0.24600  779.9670  1.89064  0.006948  1.53112  2.69800   \n",
       "4  -0.003259  3.71542  156.1280  2.14772  0.018284  2.09859  4.15492   \n",
       "\n",
       "          f7       f8        f9  ...      f91       f92       f93       f94  \\\n",
       "id                               ...                                          \n",
       "0   2.266270  2.09959  0.012330  ...  1.09862  0.013331 -0.011715  0.052759   \n",
       "1  -0.471318  4.54594  0.037706  ...  3.46017  0.017054  0.124863  0.154064   \n",
       "2   3.849460  1.80187  0.056995  ...  4.88300  0.085222  0.032396  0.116092   \n",
       "3   4.517330  4.50332  0.123494  ...  3.47439 -0.017103 -0.008100  0.062013   \n",
       "4  -0.038236  3.37145  0.034166  ...  1.91059 -0.042943  0.105616  0.125072   \n",
       "\n",
       "         f95       f96      f97       f98       f99  target  \n",
       "id                                                           \n",
       "0   0.065400  4.211250  1.97877  0.085974  0.240496       0  \n",
       "1   0.606848 -0.267928  2.57786 -0.020877  0.024719       0  \n",
       "2  -0.001688 -0.520069  2.14112  0.124464  0.148209       0  \n",
       "3   0.041193  0.511657  1.96860  0.040017  0.044873       0  \n",
       "4   0.037509  1.043790  1.07481 -0.012819  0.072798       1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = pd.read_csv('./data/train.csv').set_index('id')\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3af10f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialize_with_zeros(n_feat):\n",
    "    '''\n",
    "    This function initializes the model parameters for Logistic Regression with a given dimension of \n",
    "    number of training features: w = weights and b = bias\n",
    "    '''\n",
    "    w = np.zeros((n_feat,1))\n",
    "    b = 0.\n",
    "    return w,b\n",
    "\n",
    "def _sigmoid(z):\n",
    "    '''\n",
    "    This function computes the sigmoid function for a given inpiut z\n",
    "    '''\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def _propagation(w, b, X, Y):\n",
    "    '''\n",
    "    This function first performs the forward propagation step of Logistic Regression to compute:\n",
    "    Z = wT.X + b,\n",
    "    a = sigmoid(Z),\n",
    "    J (cost function) = 1/m.sum(L(ai, yi)) = -1/m(yi.log(ai)+(1-yi).log(1-ai))\n",
    "    \n",
    "    NOTE: X must be of the shape (nx, m) and Y must be of the shape (1xm)\n",
    "    \n",
    "    Then, it performs backward propagation to compute:\n",
    "    dw = 1/m.(X.dZ.T)\n",
    "    db = 1/m.sum(dZ)\n",
    "    where dZ = a - Y\n",
    "    \n",
    "    This funtion returns J, dw and db which will be used in the optimization step to perform gradient descent\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    a = _sigmoid(Z)\n",
    "    cost = (-1/m)*(np.sum((Y*np.log(a)) + ((1-Y)*np.log(1-a))))\n",
    "    \n",
    "    dZ = a - Y\n",
    "    dw = 1/m*(np.dot(X,dZ.T))\n",
    "    db = 1/m*np.sum(dZ)\n",
    "    \n",
    "    gradients = {'dw': dw,\n",
    "                'db': db}\n",
    "    return gradients, cost\n",
    "\n",
    "def _model_optimization(w, b, X, Y, learning_rate, iterations):\n",
    "    '''\n",
    "    This function performs gradient descent to find the optimal values of the model parameters w and b using the \n",
    "    following update steps:\n",
    "    w := w - learning_rate*dw\n",
    "    b := b - learning_rate*b\n",
    "    These upate steps are run \"iterations\" times, unlike the version of gradient descent where we have a \n",
    "    stopping criteria of (no. of iterations is determined by) minimal change in cost function.\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    '''\n",
    "    costs = []\n",
    "    #For each step of Gradient Descent Algorithm, compute the gradients and cost values and update w and b.\n",
    "    #Stop when stopping criteria is reached.\n",
    "    for i in range(iterations):\n",
    "        #Compute gradients and cost:\n",
    "        gradients, cost = _propagation(w, b, X, Y)\n",
    "        dw = gradients[\"dw\"] #From the dictionary\n",
    "        db = gradients[\"db\"] #From the dictionary\n",
    "        \n",
    "        #Update w and b:\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        #Record the costs and print after every 100 iterations:\n",
    "        if(i % 100==0):\n",
    "            costs.append(cost)\n",
    "            print(f'Cost after iteration {i} is {cost}')\n",
    "            \n",
    "    params = {'w': w,\n",
    "             'b': b}\n",
    "    \n",
    "    gradients = {'dw': dw,\n",
    "                'db': db}\n",
    "    \n",
    "    return params, gradients, costs\n",
    "\n",
    "def _predict(w, b, X):\n",
    "    '''\n",
    "    This function uses the optimized values of model parameters w, b to predict y_hat = a for each test example.\n",
    "    \n",
    "    y_hat = a = sigmoid(z)\n",
    "    \n",
    "    y_hat = 0 if a <= 0.5, else 1. (0.5 is the threshold value to determine the class)\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    y_hat = np.zeros((1,m))\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    a = _sigmoid(Z)\n",
    "    \n",
    "    for i in range(a.shape[1]):\n",
    "        y_hat[0][i] = 1 if a[0][i] > 0.5 else 0\n",
    "    return y_hat\n",
    "\n",
    "def _model(X_train, Y_train, X_test, Y_test, learning_rate, iterations):\n",
    "    '''\n",
    "    This function combines all the pieces above to create the Logistic Regression model with Gradient Descent.\n",
    "    lea\n",
    "    Returns: dictionary containing all the information about the model - costs, y_hat_train, y_hat_test, \n",
    "             w, b, learning_rate, iterations\n",
    "    '''\n",
    "    \n",
    "    #Step 1: Initialize parameters with zeros:\n",
    "    m_train = X_train.shape[1]\n",
    "    n_feat = X_train.shape[0]\n",
    "    \n",
    "    w, b = _initialize_with_zeros(n_feat)\n",
    "    print(f\"Step 1: b = {b}\")\n",
    "    \n",
    "    #Step 2: Run fwd and backward propagation steps + gradient descent to learn the parameters w, b:\n",
    "    params, gradients, costs = _model_optimization(w, b, X_train, Y_train, learning_rate, iterations)\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    print(f\"Step 2: b = {b}\")\n",
    "\n",
    "    #Step 3: Use the learned w, b from Step 2 to make predictions on training and test datasets:\n",
    "    y_pred_train = _predict(w, b, X_train)\n",
    "    y_pred_test = _predict(w, b, X_test)\n",
    "    \n",
    "    #Print tarining/test errors (mean absolute error):\n",
    "    train_error = np.mean(np.abs(Y_train - y_pred_train))*100\n",
    "    test_error = np.mean(np.abs(Y_test - y_pred_test))*100\n",
    "    train_accuracy = 100 - train_error\n",
    "    test_accuracy = 100 - test_error\n",
    "    print('Step 3:')\n",
    "    print(f'Train accuracy = {train_accuracy}')\n",
    "    print(f'Test accuracy = {test_accuracy}')\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"y_hat_train\": y_pred_train,\n",
    "         \"y_hat_test\": y_pred_test,\n",
    "         \"w\": w,\n",
    "         \"b\": b,\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"iterations\": iterations}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f27549ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: b = 0.0\n",
      "Cost after iteration 0 is 0.6931471805599452\n",
      "Cost after iteration 100 is 0.6928911155893501\n",
      "Cost after iteration 200 is 0.6928288536420201\n",
      "Cost after iteration 300 is 0.6927981111986126\n",
      "Step 2: b = 2.363623692969738e-06\n",
      "Step 3:\n",
      "Train accuracy = 50.990545454545455\n",
      "Test accuracy = 50.792\n"
     ]
    }
   ],
   "source": [
    "#Split dataset into training and test data:\n",
    "#First shuffle the input_data df and then split into 550k and 50k\n",
    "input_df_shuffled = input_df.iloc[np.random.permutation(input_df.shape[0])].reset_index(drop = True)\n",
    "train_data = input_df_shuffled.loc[:550000-1]\n",
    "# print(train_data.shape)\n",
    "# print(train_data.head())\n",
    "test_data = input_df_shuffled.loc[550000:]\n",
    "# print(test_data.shape)\n",
    "\n",
    "#Get the matrices X_train and X_test in the shape of (nx, m) and y)train and y_test in the hshape of (1, m):\n",
    "X_train = train_data.drop(columns='target').T\n",
    "# print(X_train.shape)\n",
    "y_train = np.array(train_data[['target']].T)\n",
    "# print(type(y_train))\n",
    "\n",
    "X_test = test_data.drop(columns='target').T\n",
    "# print(X_test.shape)\n",
    "y_test = np.array(test_data[['target']].T)\n",
    "# print(y_test.shape)\n",
    "\n",
    "#Run Logistic Regression and print the output:\n",
    "logistic_regression = _model(X_train, y_train, X_test, y_test, learning_rate = 0.000001, iterations = 305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ceb96f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'costs': [0.6931471805599452,\n",
       "  0.6928911155893501,\n",
       "  0.6928288536420201,\n",
       "  0.6927981111986126],\n",
       " 'y_hat_train': array([[1., 0., 1., ..., 0., 1., 1.]]),\n",
       " 'y_hat_test': array([[1., 1., 0., ..., 0., 1., 1.]]),\n",
       " 'w': array([[ 8.66824018e-07],\n",
       "        [-1.12280638e-07],\n",
       "        [-5.79875000e-05],\n",
       "        [-3.41361751e-06],\n",
       "        [ 2.40002759e-06],\n",
       "        [-8.25142907e-07],\n",
       "        [ 8.32607245e-06],\n",
       "        [ 9.00937905e-06],\n",
       "        [ 3.06183680e-05],\n",
       "        [-1.57368051e-06],\n",
       "        [ 1.49932891e-05],\n",
       "        [ 9.86394063e-06],\n",
       "        [ 5.36921573e-07],\n",
       "        [ 6.83392882e-06],\n",
       "        [ 2.40892002e-06],\n",
       "        [ 3.42436930e-06],\n",
       "        [-3.26797447e-06],\n",
       "        [-2.82513484e-06],\n",
       "        [ 8.19742529e-06],\n",
       "        [ 1.22791532e-06],\n",
       "        [ 1.86611892e-06],\n",
       "        [-3.14496924e-07],\n",
       "        [-9.35664359e-06],\n",
       "        [-4.76912262e-07],\n",
       "        [ 2.90248071e-06],\n",
       "        [-1.05731310e-05],\n",
       "        [-3.28889285e-06],\n",
       "        [ 1.88628832e-06],\n",
       "        [ 1.62086841e-06],\n",
       "        [ 6.78915682e-06],\n",
       "        [ 1.85147098e-06],\n",
       "        [-1.63684407e-06],\n",
       "        [ 1.92993605e-06],\n",
       "        [ 4.93716560e-07],\n",
       "        [ 3.80399709e-05],\n",
       "        [ 2.27234169e-04],\n",
       "        [-5.06112682e-07],\n",
       "        [ 3.48267526e-06],\n",
       "        [ 5.61467418e-06],\n",
       "        [ 9.88646266e-07],\n",
       "        [ 1.91854529e-05],\n",
       "        [ 2.44169165e-05],\n",
       "        [ 1.45353653e-06],\n",
       "        [ 3.21977157e-05],\n",
       "        [ 3.89349507e-05],\n",
       "        [ 1.14911414e-05],\n",
       "        [ 2.51945548e-07],\n",
       "        [-2.42190552e-06],\n",
       "        [-9.94682942e-07],\n",
       "        [-2.42656886e-06],\n",
       "        [ 2.56625676e-05],\n",
       "        [ 1.62075819e-06],\n",
       "        [ 8.24447550e-07],\n",
       "        [-6.13379532e-07],\n",
       "        [-3.86578705e-06],\n",
       "        [-1.96940272e-05],\n",
       "        [ 2.91787046e-06],\n",
       "        [ 2.27401610e-05],\n",
       "        [ 1.46887521e-06],\n",
       "        [-3.57802280e-07],\n",
       "        [ 1.30577761e-06],\n",
       "        [ 1.08653347e-06],\n",
       "        [-1.26527002e-06],\n",
       "        [ 5.11411661e-07],\n",
       "        [-6.62680569e-07],\n",
       "        [ 5.75830434e-06],\n",
       "        [-9.30084849e-06],\n",
       "        [ 6.35854448e-07],\n",
       "        [ 1.02636923e-06],\n",
       "        [ 1.93909078e-06],\n",
       "        [ 1.00821898e-05],\n",
       "        [-1.92033165e-05],\n",
       "        [ 2.07733234e-07],\n",
       "        [ 1.59766393e-07],\n",
       "        [ 7.68338804e-06],\n",
       "        [-1.14126347e-06],\n",
       "        [-1.96501962e-06],\n",
       "        [ 1.00041447e-05],\n",
       "        [-1.97623302e-07],\n",
       "        [ 1.69913392e-06],\n",
       "        [-1.96999355e-05],\n",
       "        [ 1.39954852e-05],\n",
       "        [-6.92460282e-06],\n",
       "        [-6.86136786e-07],\n",
       "        [-3.56115247e-06],\n",
       "        [ 7.48465751e-06],\n",
       "        [ 4.41305596e-06],\n",
       "        [ 2.44895869e-07],\n",
       "        [ 2.82973118e-06],\n",
       "        [ 2.81761197e-07],\n",
       "        [-1.56525710e-07],\n",
       "        [-1.91384870e-05],\n",
       "        [-2.76348306e-09],\n",
       "        [ 3.37788123e-08],\n",
       "        [-1.91813342e-06],\n",
       "        [ 1.22960259e-06],\n",
       "        [ 2.01533547e-05],\n",
       "        [-1.26454305e-05],\n",
       "        [ 3.49016087e-06],\n",
       "        [-1.77948893e-07]]),\n",
       " 'b': 2.363623692969738e-06,\n",
       " 'learning_rate': 1e-06,\n",
       " 'iterations': 305}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9372e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10477a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba29cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be6ae66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[ 0.25071532]\n",
      " [-0.06604096]]\n",
      "db = -0.1250040450043965\n",
      "cost = 0.15900537707692405\n"
     ]
    }
   ],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "gradients, cost = _propagation(w, b, X, Y)\n",
    "\n",
    "assert type(gradients[\"dw\"]) == np.ndarray\n",
    "assert gradients[\"dw\"].shape == (2, 1)\n",
    "assert type(gradients[\"db\"]) == np.float64\n",
    "\n",
    "\n",
    "print (\"dw = \" + str(gradients[\"dw\"]))\n",
    "print (\"db = \" + str(gradients[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2227a379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 is 0.15900537707692405\n",
      "Cost after iteration 100 is 0.10541138368032707\n",
      "w = [[0.80795802]\n",
      " [2.05125464]]\n",
      "b = 1.5956687730718366\n",
      "dw = [[ 0.178049  ]\n",
      " [-0.04827102]]\n",
      "db = -0.08860601121976527\n",
      "Costs = [0.15900537707692405, 0.10541138368032707]\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = _model_optimization(w, b, X, Y, iterations=101, learning_rate=0.009)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae020a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(_predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf2623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cdcc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a43a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acee18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086f0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca7abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a0f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
